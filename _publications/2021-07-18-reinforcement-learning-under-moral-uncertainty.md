---
title: "Reinforcement Learning Under Moral Uncertainty"
collection: publications
permalink: /publication/2021-07-18-reinforcement-learning-under-moral-uncertainty
excerpt: 'An ambitious goal for artificial intelligence is to create agents that behave ethically. Unfortunately, there is widespread disagreement about which ethical theory an agent should follow. This paper translates philosophical work on moral uncertainty into an RL algorithm and investigates how moral uncertainty can avoid extreme behavior compared to single theories.'
date: 2021-07-18
venue: 'ICML 2021'
paperurl: 'https://arxiv.org/pdf/2006.04734.pdf'
authors: '<span class="me">Adrien Ecoffet</span>, Joel Lehman'
---
An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.

[Download paper here](https://arxiv.org/pdf/2006.04734.pdf)

You can cite this work using the BibTeX from Google Scholar, which should be as follows:
```
@inproceedings{ecoffet2021reinforcement,
  title={Reinforcement learning under moral uncertainty},
  author={Ecoffet, Adrien and Lehman, Joel},
  booktitle={International Conference on Machine Learning},
  pages={2926--2936},
  year={2021},
  organization={PMLR}
}
```